{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trexquant Interview Project (The Hangman Game)\n",
    "\n",
    "* Copyright Trexquant Investment LP. All Rights Reserved. \n",
    "* Redistribution of this question without written consent from Trexquant is prohibited"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instruction:\n",
    "For this coding test, your mission is to write an algorithm that plays the game of Hangman through our API server. \n",
    "\n",
    "When a user plays Hangman, the server first selects a secret word at random from a list. The server then returns a row of underscores (space separated)—one for each letter in the secret word—and asks the user to guess a letter. If the user guesses a letter that is in the word, the word is redisplayed with all instances of that letter shown in the correct positions, along with any letters correctly guessed on previous turns. If the letter does not appear in the word, the user is charged with an incorrect guess. The user keeps guessing letters until either (1) the user has correctly guessed all the letters in the word\n",
    "or (2) the user has made six incorrect guesses.\n",
    "\n",
    "You are required to write a \"guess\" function that takes current word (with underscores) as input and returns a guess letter. You will use the API codes below to play 1,000 Hangman games. You have the opportunity to practice before you want to start recording your game results.\n",
    "\n",
    "Your algorithm is permitted to use a training set of approximately 250,000 dictionary words. Your algorithm will be tested on an entirely disjoint set of 250,000 dictionary words. Please note that this means the words that you will ultimately be tested on do NOT appear in the dictionary that you are given. You are not permitted to use any dictionary other than the training dictionary we provided. This requirement will be strictly enforced by code review.\n",
    "\n",
    "You are provided with a basic, working algorithm. This algorithm will match the provided masked string (e.g. a _ _ l e) to all possible words in the dictionary, tabulate the frequency of letters appearing in these possible words, and then guess the letter with the highest frequency of appearence that has not already been guessed. If there are no remaining words that match then it will default back to the character frequency distribution of the entire dictionary.\n",
    "\n",
    "This benchmark strategy is successful approximately 18% of the time. Your task is to design an algorithm that significantly outperforms this benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import random\n",
    "import string\n",
    "import secrets\n",
    "import time\n",
    "import re\n",
    "import collections\n",
    "try:\n",
    "    from urllib.parse import parse_qs, urlencode, urlparse\n",
    "except ImportError:\n",
    "    from urlparse import parse_qs, urlparse\n",
    "    from urllib import urlencode\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import entropy\n",
    "from IPython.core.debugger import Pdb\n",
    "from functools import reduce\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class HangmanPlayer(object):\n",
    "    def __init__(self):\n",
    "        self.guessed_letters = []\n",
    "        \n",
    "        full_dictionary_location = \"words_250000_train.txt\"\n",
    "        self.full_dictionary = self.build_dictionary(full_dictionary_location)  \n",
    "        self.full_dictionary_common_letter_sorted = collections.Counter(\"\".join(self.full_dictionary)).most_common()\n",
    "\n",
    "        target_dictionary_location = \"words_final_test.txt\"\n",
    "        self.target_dictionary = self.build_dictionary(target_dictionary_location)  \n",
    "        \n",
    "    def reset(self,length):\n",
    "        \"\"\"\n",
    "        Resets the probability estimates to the full dictionary before a word is started        \n",
    "        \"\"\"\n",
    "        max_length = length\n",
    "        self.prob_matrix = []\n",
    "        self.dictionary_i = []\n",
    "        for i in range(max_length):\n",
    "            # Estimate positional probabilities\n",
    "            self.prob_matrix.append(self.letter_distribution_at_place(self.full_dictionary,i)) \n",
    "            # Append the dictionary for each index which is being used in the previous estimate\n",
    "            self.dictionary_i.append(self.full_dictionary)\n",
    "            \n",
    "\n",
    "    def letter_distribution(self,dictionary): \n",
    "        \"\"\"\n",
    "        Returns the probability distribution of letters given words in a dictionary     \n",
    "        \n",
    "        \"\"\"\n",
    "        dict_string = \"\".join(dictionary)        \n",
    "        c = collections.Counter(dict_string)\n",
    "        letter_count = dict(c.most_common()) \n",
    "        for letter in string.ascii_lowercase:\n",
    "            if letter in letter_count:\n",
    "                letter_count[letter] = (letter_count[letter]+1)/(len(dict_string)+26)\n",
    "            else:\n",
    "                letter_count[letter] = 1/26\n",
    "        return letter_count\n",
    "    \n",
    "    def unique_letter_distribution(self,dictionary):\n",
    "        \"\"\"\n",
    "        Returns the probability distribution of unique letters ('aaa will resolve to a') for dictionary\n",
    "        \n",
    "        \"\"\"\n",
    "        remove_duplicates = lambda s: ''.join(sorted(set(s), key=s.index))\n",
    "        dictionary = [remove_duplicates(word) for word in dictionary]\n",
    "        return self.letter_distribution(dictionary)\n",
    "        \n",
    "    def letter_distribution_at_place(self,dictionary,i):\n",
    "        \"\"\"\n",
    "        Returns the letter distribution at position i in all words\n",
    "        \n",
    "        \"\"\"\n",
    "        dictionary = [word[i] for word in dictionary if len(word) > i]\n",
    "        return self.letter_distribution(dictionary)\n",
    "        \n",
    "    def guess(self, word): # word input example: \"_ p p _ e \"\n",
    "        clean_word = word[::2].replace(\"_\",\".\")\n",
    "        len_word = len(clean_word)\n",
    "        \n",
    "        self.update_prob_matrix(clean_word) # Update the prob matrix\n",
    "        self.prev_clean_word = clean_word \n",
    "              \n",
    "        guess_letter = '!'\n",
    "        \n",
    "        \n",
    "        # Here I sum up the prob vectors to get the expected number of hits for all letters and then\n",
    "        # I choose the maximum\n",
    "        prob_vector = reduce(lambda a,b : dict(collections.Counter(a)+collections.Counter(b)),self.prob_matrix[:len_word])\n",
    "        prob_vector = collections.Counter(prob_vector).most_common()\n",
    "        for letter,probability in prob_vector:\n",
    "            if letter not in self.guessed_letters:\n",
    "                guess_letter = letter\n",
    "                break\n",
    "\n",
    "        if guess_letter == '!':\n",
    "            sorted_letter_count = self.full_dictionary_common_letter_sorted\n",
    "            for letter,instance_count in sorted_letter_count:\n",
    "                if letter not in self.guessed_letters:\n",
    "                    guess_letter = letter\n",
    "                    break  \n",
    "                                \n",
    "        return guess_letter\n",
    "    \n",
    "    def update_prob_matrix(self,clean_word):\n",
    "        \n",
    "        def regex_to_match(word,i,c):\n",
    "            \"\"\"\n",
    "            Build a regex to match for the word for a position i and c context window\n",
    "            This regex is used to filter the dictionary for the new probability estimates\n",
    "            \"\"\"   \n",
    "            if (i == 0) or (i == len(word)-1):\n",
    "                c = c+1\n",
    "            start = max(i-c,0)\n",
    "            end = min(i+c+1,len(word))\n",
    "            regex = \".\"*start + word[start:end]+\".*\"*(end < len(word))\n",
    "            return regex   \n",
    "        \n",
    "        char_list = list(clean_word)\n",
    "        singleton_dict = dict([(letter,0) for letter in string.ascii_lowercase])\n",
    "        context = 2\n",
    "        for i,elem in enumerate(char_list):\n",
    "            if elem != \".\": # Already guessed, assign 1 probabiltiy to this letter for this index\n",
    "                self.prob_matrix[i] = singleton_dict.copy()\n",
    "                self.prob_matrix[i][elem] = 1\n",
    "            else:\n",
    "                filtered_dict = self.dictionary_i[i]\n",
    "                regex = regex_to_match(clean_word,i,context) \n",
    "                prev_regex = regex_to_match(self.prev_clean_word,i,context)\n",
    "                if regex != prev_regex: # Some new word has been guessed in the context window\n",
    "                    filtered_dict = list(filter(lambda word: re.match(regex,word) is not None,\n",
    "                                                    self.dictionary_i[i]))      \n",
    "                if len(filtered_dict)<len(self.dictionary_i[i]):\n",
    "                    self.prob_matrix[i] = self.letter_distribution_at_place(filtered_dict,i) # Estimate new\n",
    "                    self.dictionary_i[i] = filtered_dict # Update the dictionary for this estimate\n",
    "        return\n",
    "    \n",
    "    def build_dictionary(self, dictionary_file_location):\n",
    "        text_file = open(dictionary_file_location,\"r\")\n",
    "        full_dictionary = text_file.read().splitlines()\n",
    "        text_file.close()\n",
    "        return full_dictionary\n",
    "                \n",
    "    \n",
    "    \n",
    "    def start_game(self,verbose):\n",
    "        self.guessed_letters = []\n",
    "        \n",
    "        target_word = self.target_dictionary[np.random.randint(len(self.target_dictionary))]\n",
    "        #if len(target_word)<11:\n",
    "        #    target_word = self.target_dictionary[np.random.randint(len(self.target_dictionary))]\n",
    "        \n",
    "        word = \"_\"*len(target_word)\n",
    "        self.reset(len(word))\n",
    "        self.prev_clean_word = word\n",
    "        \n",
    "        self.TARGET = target_word\n",
    "        \n",
    "        self.incorrect_guesses = []\n",
    "        self.current_dictionary = self.full_dictionary\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"Word is \",target_word)\n",
    "            print(\"Successfully start a new game!\")\n",
    "           \n",
    "        tries_remains = 6\n",
    "        \n",
    "        while tries_remains>0:\n",
    "            guess_letter = self.guess(word,tries_remains)\n",
    "            self.guessed_letters.append(guess_letter)\n",
    "            if verbose:\n",
    "                print(\"Guessing letter: {0}\".format(guess_letter))\n",
    "            \n",
    "            updated_word,correct = self.update_word(target_word,guess_letter,word)\n",
    "            if correct == False:\n",
    "                tries_remains = tries_remains -1 \n",
    "                self.incorrect_guesses.append(guess_letter)\n",
    "                if verbose:\n",
    "                    print(\"Incorrect guess\")\n",
    "                    print(\"Tries remaining \",tries_remains)\n",
    "                    \n",
    "            if updated_word == target_word:\n",
    "                if verbose:\n",
    "                    print(\"Game won\")\n",
    "                return 1\n",
    "            word = updated_word\n",
    "        if verbose:\n",
    "            print(\"Couldnt guess\")\n",
    "        return 0\n",
    "        \n",
    "    def update_word(self,target_word,guess_letter,word):\n",
    "        if guess_letter in target_word:\n",
    "            temp_word = ''.join(map(lambda char: char if char == guess_letter else '_', target_word))\n",
    "            union = \"\"\n",
    "            for c1, c2 in zip(temp_word, word):\n",
    "                if c1 != \"_\":\n",
    "                    union += c1\n",
    "                else:\n",
    "                    union += c2\n",
    "            return(union,1)\n",
    "        return (word,0)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'str' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m losing_words \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(total_games):\n\u001b[0;32m----> 9\u001b[0m     won \u001b[38;5;241m=\u001b[39m \u001b[43mplayer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart_game\u001b[49m\u001b[43m(\u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m     total_won \u001b[38;5;241m=\u001b[39m total_won \u001b[38;5;241m+\u001b[39m won\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m won:\n",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36mHangmanPlayer.start_game\u001b[0;34m(self, verbose)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;66;03m#if len(target_word)<11:\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;66;03m#    target_word = self.target_dictionary[np.random.randint(len(self.target_dictionary))]\u001b[39;00m\n\u001b[1;32m    134\u001b[0m word \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlen\u001b[39m(target_word)\n\u001b[0;32m--> 135\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprev_clean_word \u001b[38;5;241m=\u001b[39m word\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mTARGET \u001b[38;5;241m=\u001b[39m target_word\n",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36mHangmanPlayer.reset\u001b[0;34m(self, length)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprob_matrix \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdictionary_i \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;66;03m# Estimate positional probabilities\u001b[39;00m\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprob_matrix\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mletter_distribution_at_place(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfull_dictionary,i)) \n\u001b[1;32m     22\u001b[0m     \u001b[38;5;66;03m# Append the dictionary for each index which is being used in the previous estimate\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'str' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "np.random.seed(29)\n",
    "player = HangmanPlayer()\n",
    "total_won = 0\n",
    "total_games = 500\n",
    "\n",
    "winning_words = []\n",
    "losing_words = []\n",
    "for i in range(total_games):\n",
    "    won = player.start_game(verbose=False)\n",
    "    total_won = total_won + won\n",
    "    if won:\n",
    "        winning_words.append(player.TARGET)\n",
    "    else:\n",
    "        losing_words.append(player.TARGET)\n",
    "    print(total_won/(i+1))\n",
    "    #print(won)\n",
    "    \n",
    "#print(total_won/total_games)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.51340206185567"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_won/(i+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "484"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['drinn',\n",
       " 'fistnotes',\n",
       " 'denizenize',\n",
       " 'rattle',\n",
       " 'counterstratagem',\n",
       " 'wavira',\n",
       " 'overdraw',\n",
       " 'quodlibetic',\n",
       " 'monkly',\n",
       " 'deskman',\n",
       " 'ungothic',\n",
       " 'animalculae',\n",
       " 'cucujus',\n",
       " 'tophus',\n",
       " 'falsen',\n",
       " 'reliving',\n",
       " 'hunterlike',\n",
       " 'alsoon',\n",
       " 'bumfuzzle',\n",
       " 'steelbow',\n",
       " 'epicalyxes',\n",
       " 'sitcom',\n",
       " 'wightness',\n",
       " 'peacockish',\n",
       " 'vowelless',\n",
       " 'gerbilles',\n",
       " 'befitted',\n",
       " 'drunkenness',\n",
       " 'lithuria',\n",
       " 'democracy',\n",
       " 'repiece',\n",
       " 'tagassuidae',\n",
       " 'interrepulsion',\n",
       " 'stowwood',\n",
       " 'syrianic',\n",
       " 'enzymotic',\n",
       " 'bambos',\n",
       " 'unanalagous',\n",
       " 'enjoiner',\n",
       " 'bullshots',\n",
       " 'valsaceae',\n",
       " 'abjuratory',\n",
       " 'sieur',\n",
       " 'antoinette']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losing_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "92"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'...sds'"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = \"asdsds\"\n",
    "\".\"*(len(word)-3) + word[len(word)-3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The string does not contain 'h', 'y', 'x', or 'z'\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "string = \"ello, World! Tis string contains'.\"\n",
    "pattern = \"[hyxz]\"\n",
    "\n",
    "if re.search(pattern, string):\n",
    "    print(\"The string contains 'h', 'y', 'x', or 'z'\")\n",
    "else:\n",
    "    print(\"The string does not contain 'h', 'y', 'x', or 'z'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    def conditional_entropy(self):\n",
    "        conditional_entropy = dict()\n",
    "        for letter in string.ascii_lowercase:\n",
    "            matching_dictionary = list(filter(lambda word: letter in word,self.current_dictionary))\n",
    "            non_matching_dictionary = list(filter(lambda word: letter not in word,self.current_dictionary))            \n",
    "            match_prob = len(matching_dictionary)/len(self.current_dictionary)\n",
    "            non_match_prob = 1- match_prob\n",
    "            match_letter_count = self.unique_letter_distribution(matching_dictionary)\n",
    "            non_match_letter_count = self.unique_letter_distribution(non_matching_dictionary)\n",
    "            match_entropy = entropy(list(match_letter_count.values()))\n",
    "            non_match_entropy = entropy(list(non_match_letter_count.values()))\n",
    "            conditional_entropy[letter] = match_prob*match_entropy + non_match_prob*non_match_entropy           \n",
    "        return sorted(conditional_entropy.items(), key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regex_to_match(word,i,c):\n",
    "    start = max(i-c,0)\n",
    "    end = min(i+c+1,len(word))\n",
    "    regex = \".\"*start + word[start:end]+\".*\"*(end < len(word))\n",
    "    return regex\n",
    "    if i == 0:\n",
    "        regex = \".\" + word[i+1] + \".*\" \n",
    "    elif i == len(word)-1:\n",
    "        regex = \".\"*(len(word)-2) + word[i-1:]\n",
    "    else:\n",
    "        regex = \".\"*(i-1) + word[i-1:i+2] + \".*\"\n",
    "\n",
    "    return regex\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'.....ful'"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regex_to_match(\"blissful\",7,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "incorrect_regex = \"r[\" + \"\".join([\"a\"]) + \"]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'r[a]'"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "incorrect_regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "if re.search(incorrect_regex,\"e\") is None:\n",
    "    print(\"1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "979"
      ]
     },
     "execution_count": 399,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
